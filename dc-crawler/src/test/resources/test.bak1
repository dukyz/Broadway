import java.io.File
import java.net.{InetSocketAddress, URLEncoder}
import javax.xml.xpath.XPathFactory

import akka.actor.ActorSystem
import akka.http.scaladsl.model.HttpRequest
import akka.http.scaladsl.settings.{ClientConnectionSettings, ConnectionPoolSettings}
import akka.http.scaladsl.{ClientTransport, Http}
import akka.stream.scaladsl.StreamConverters
import akka.stream.{ActorMaterializer, Materializer}
import com.typesafe.config.ConfigFactory
import edu.uci.ics.crawler4j.crawler.{Page, WebCrawler}
import edu.uci.ics.crawler4j.url.WebURL
import org.apache.tika.metadata.Metadata
import org.apache.tika.parser.AutoDetectParser
import org.apache.tika.parser.html.HtmlParser
import org.apache.tika.sax.xpath.{MatchingContentHandler, XPathParser}
import org.apache.tika.sax.{BodyContentHandler, ToHTMLContentHandler, ToXMLContentHandler, XHTMLContentHandler}
import org.dom4j.Element
import org.dom4j.io.{DOMReader, SAXReader}
import org.htmlcleaner.{CleanerProperties, DomSerializer, HtmlCleaner, TagNode}

import scala.concurrent.ExecutionContext
import scala.concurrent.duration._
import scala.collection.JavaConversions._

object MainTest extends App {
    val config = ConfigFactory.load()
    val path = "default.actor.crawler.KeywordDispatcher.searchengine"
    implicit val actorSystem = ActorSystem.create("default",config)
    implicit val executeContent = actorSystem.dispatcher
    implicit val materializer = ActorMaterializer()

    val KEYWORD = URLEncoder.encode("笑匠","utf8")
    var url = s"http://www.baidu.com"
    url = "https://baike.baidu.com/item/%E7%AC%91%E5%8C%A0/13856647?fr=aladdin"
    url = s"http://www.baidu.com/s?q1=$KEYWORD&rn=50&lm=1"
    val x = download(url)

    x.foreach( ip => {

        val xhtmlParser = new XPathParser("xhtml", XHTMLContentHandler.XHTML)
        val divContentMatcher = xhtmlParser.parse("//a/@href")
        val handler = new MatchingContentHandler(new ToXMLContentHandler, divContentMatcher)
//        val handler = new ToHTMLContentHandler()
//        val parser = new HtmlParser()
//        val metadata = new Metadata
//        parser.parse(ip, handler, metadata)
//        println(handler.toString)
//        println(metadata.toString)
//        val hc = new HtmlCleaner()
//        val tagNode = hc.clean(ip).evaluateXPath("//a/@href")
//        tagNode.foreach(x => println(x.asInstanceOf[String]))
//        val doc = new DomSerializer(new CleanerProperties()).createDOM(tagNode)
//        println(ip)
//        val saxReader = new SAXReader()
//        saxReader.read(ip).selectNodes("//a/@href").listIterator.foreach(println)

//        val domReader = new DOMReader
//        val contactElems = domReader.read(doc).selectNodes("//a/@href").listIterator()
//        contactElems.foreach( x => println(x.getText))
//        ip.close()
    })


    def download(targetUri:String,proxyHost:String=null,proxyPort:Int=18080)
                (implicit actorSystem: ActorSystem,executeContent:ExecutionContext,materializer:Materializer)= {
        println(targetUri)
        val settings = ConnectionPoolSettings(actorSystem)
            .withMaxRetries(2)
            .withConnectionSettings(
                ClientConnectionSettings(actorSystem)
                    .withConnectingTimeout(2.seconds)
            )
        Http().singleRequest(HttpRequest(uri = targetUri),settings=settings).map( resp =>
            resp.entity.dataBytes.runWith(StreamConverters.asInputStream(0.seconds))
        )
    }

//    import edu.uci.ics.crawler4j.crawler.CrawlConfig
//    import edu.uci.ics.crawler4j.fetcher.PageFetcher
//    import edu.uci.ics.crawler4j.crawler.CrawlController
//    import edu.uci.ics.crawler4j.robotstxt.RobotstxtConfig
//    import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer
//
//    val config = new CrawlConfig
//    config.setCrawlStorageFolder("/tmp/crawler4j")
//    val pageFetcher = new PageFetcher(config)
//    val robotstxtConfig = new RobotstxtConfig
//    robotstxtConfig.setEnabled(false)
//    val robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher)
//
//    val controller = new CrawlController(config, pageFetcher, robotstxtServer)
//    controller.addSeed("http://www.baidu.com/")
//    controller.start(classOf[myCrawler], 1)
//
//
//
//
//    class myCrawler extends WebCrawler {
//
//        override def shouldVisit(referringPage:Page,url:WebURL):Boolean = {
//            println("11111"+url.getURL)
//            false
//        }
//
//        override def visit(page:Page):Unit = {
//            val url = page.getWebURL.getURL
//            println("22222"+url)
//            import edu.uci.ics.crawler4j.parser.HtmlParseData
//            if (page.getParseData.isInstanceOf[HtmlParseData]) {
//                val htmlParseData = page.getParseData.asInstanceOf[HtmlParseData]
//
//
//
//                val text = htmlParseData.getText
//                val html = htmlParseData.getHtml
//                val links = htmlParseData.getOutgoingUrls
//                println(html)
//            }
//
//
//
//        }
//    }



//    val hc = new HtmlCleaner()
//    val tagNode = hc.clean(new File("/home/dukyz/Downloads/baike1.html"))
//    val doc1 = new DomSerializer(
//        new CleanerProperties()).createDOM(tagNode);
//
//    import javax.xml.xpath.XPathConstants
//
//
//
//    val saxReader = new SAXReader()
//
//
//    import org.dom4j.io.DOMReader
//
//    val xmlReader = new DOMReader
//
////    val doc = saxReader.read(new File("/home/dukyz/Downloads/example.html"))
//    val doc = xmlReader.read(doc1)
//    val contactElems = doc.selectNodes("//a/@href").listIterator()
//    contactElems.foreach( x => println(x.getText))

}

